{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reported-azerbaijan",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endless-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-22 09:26:47,647\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: https://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "import glob, os, time, json\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(os.path.expanduser('~/Research/MyRepos/'),'SensoryMotorPred'))\n",
    "from utils import check_path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import AudioDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse, random, multiprocessing\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import icecream as ic\n",
    "\n",
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(f'Dashboard URL: https://{ray.get_dashboard_url()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "magnetic-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "savefigs=Falsedevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Trial      = 0    # Trial number\n",
    "n_channels = 1\n",
    "img_height = 128\n",
    "img_width  = 16\n",
    "stacks = (4,16) #(32, 64, 128, 256) # \n",
    "stack_sizes       = (n_channels,) + (stacks[:-1])\n",
    "R_stack_sizes     = stacks\n",
    "FiltSizes         = 3\n",
    "Ahat_filt_sizes   = tuple([FiltSizes for _ in range(len(stack_sizes))]) # (FiltSizes, FiltSizes, FiltSizes, FiltSizes)\n",
    "R_filt_sizes      = tuple([FiltSizes for _ in range(len(stack_sizes))]) # (FiltSizes, FiltSizes, FiltSizes, FiltSizes)\n",
    "\n",
    "BatchSize   = 8\n",
    "TimeSize    = 1#8\n",
    "WindSize    = 16\n",
    "Overlap     = 8\n",
    "Nepochs     = 1500   # Number of epochs (full run through the dataset)\n",
    "Tau         = 1     # Number steps to predict ##### Only implemented Tau =1 for now\n",
    "output_mode = 'error' # Type of output, 'error', 'prediction', 'all' \n",
    "rootdir = Path('~/Research/PredAudio/').expanduser()\n",
    "FigurePath = check_path(rootdir,'Figures')\n",
    "save_path   = check_path(rootdir, 'results')\n",
    "fileList    = list(save_path.glob('*params.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "instrumental-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define Network Parameters ##########\n",
    "params = {'width': img_width,\n",
    "            'height': img_height,\n",
    "            'BatchSize': int(BatchSize),\n",
    "            'TimeSize': int(TimeSize),\n",
    "            'WindSize': int(WindSize),\n",
    "            'Overlap': int(Overlap),\n",
    "            'Tau': int(Tau),\n",
    "            'FiltNum': stack_sizes[1],\n",
    "            'KSize': R_filt_sizes[0], # Filter size for Recurrent Layer\n",
    "            'stack_sizes': stack_sizes,\n",
    "            'R_stack_sizes': R_stack_sizes,\n",
    "            'Ahat_filt_sizes': Ahat_filt_sizes,\n",
    "            'R_filt_sizes': R_filt_sizes,\n",
    "            'layer_loss_weightsMode': 'L_0',\n",
    "            'lr': 0.005,\n",
    "            'Nepochs': int(Nepochs),\n",
    "            'output_mode': output_mode,\n",
    "            'data_format': 'channels_first',\n",
    "            'ImageSize': img_width*img_height,\n",
    "            'log_freq': 10, #\n",
    "            'rootdir': rootdir,\n",
    "            'save_path': save_path,\n",
    "            'input_shape':(BatchSize,TimeSize,img_height,img_width,1),\n",
    "            'Train_paths': rootdir/'Specs_train.npy',\n",
    "            'Test_paths' : rootdir/'Specs_test.npy',\n",
    "            'Trial' : int(Trial),\n",
    "            }\n",
    "\n",
    "########## Input Shape for building Network ##########\n",
    "if params['data_format'] == 'channels_first':\n",
    "    input_shape = (params['BatchSize'], params['TimeSize'], 1, params['height'], params['width'])\n",
    "else:\n",
    "    input_shape = (params['BatchSize'], params['TimeSize'], params['height'], params['width'], 1)\n",
    "\n",
    "    ########## Create Datasets and DataLoaders ##########\n",
    "Dataset_Train = AudioDataset(params['Train_paths'],params['WindSize'],params['Overlap'])\n",
    "Dataset_Test = AudioDataset(params['Test_paths'],params['WindSize'],params['Overlap'])\n",
    "num_workers = multiprocessing.cpu_count()//2\n",
    "DataLoader_Train = DataLoader(Dataset_Train, batch_size=params['BatchSize'], shuffle=True, drop_last=False, num_workers=num_workers, pin_memory=True)\n",
    "DataLoader_Test = DataLoader(Dataset_Test, batch_size=params['BatchSize'], shuffle=True, drop_last=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# optimizer = torch.optim.Adam(precnet.parameters(), lr = params['lr'])\n",
    "batch = next(iter(DataLoader_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "touched-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 16), (1, 4))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_stack_sizes,stack_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "german-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreCNet(nn.Module):\n",
    "    def __init__(self, stack_sizes, R_stack_sizes, Ahat_filt_sizes, R_filt_sizes,\n",
    "                 pixel_max=1., error_activation='relu', stateful = True,\n",
    "                 GRU_activation='tanh', GRU_inner_activation='hard_sigmoid',\n",
    "                 output_mode='error', extrap_start_time=None, data_format = 'channels_first',\n",
    "                 device='cuda',\n",
    "                 lr=.003, optimizer='Adam', **kwargs):\n",
    "        super(PreCNet, self).__init__()\n",
    "        self.stack_sizes            = stack_sizes\n",
    "        self.nb_layers              = len(stack_sizes)\n",
    "        assert len(R_stack_sizes)   == self.nb_layers, 'len(R_stack_sizes) must equal len(stack_sizes)'\n",
    "        self.R_stack_sizes          = R_stack_sizes\n",
    "        assert len(Ahat_filt_sizes) == self.nb_layers, 'len(Ahat_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.Ahat_filt_sizes        = Ahat_filt_sizes\n",
    "        assert len(R_filt_sizes)    == (self.nb_layers), 'len(R_filt_sizes) must equal len(stack_sizes)'\n",
    "        self.R_filt_sizes           = R_filt_sizes\n",
    "        self.num_layers             = len(stack_sizes)\n",
    "        self.pixel_max              = pixel_max\n",
    "        self.stateful               = stateful\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.lr = lr\n",
    "        default_output_modes = ['prediction', 'error', 'all']\n",
    "        layer_output_modes = [layer + str(n) for n in range(self.nb_layers) for layer in ['Rd', 'Ed', 'Ad', 'Ahatd','Ru', 'Eu', 'Au', 'Ahatu']]\n",
    "        assert output_mode in default_output_modes + layer_output_modes, 'Invalid output_mode: ' + str(output_mode)\n",
    "        self.output_mode = output_mode\n",
    "        if self.output_mode in layer_output_modes:\n",
    "            self.output_layer_type = self.output_mode[:-1]\n",
    "            self.output_layer_num = int(self.output_mode[-1])\n",
    "        else:\n",
    "            self.output_layer_type = None\n",
    "            self.output_layer_num = None\n",
    "        self.extrap_start_time = extrap_start_time\n",
    "\n",
    "        assert data_format in {'channels_last', 'channels_first'}, 'data_format must be in {channels_last, channels_first}'\n",
    "        self.data_format = data_format\n",
    "        self.channel_axis = -3 if data_format == 'channels_first' else -1\n",
    "        self.row_axis = -2 if data_format == 'channels_first' else -3\n",
    "        self.column_axis = -1 if data_format == 'channels_first' else -2\n",
    "        self.device=device\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.layers_u = nn.ModuleDict()\n",
    "        self.layers_d = nn.ModuleDict()\n",
    "        self.layers_a = nn.ModuleDict()\n",
    "        for l in range(nb_layers):\n",
    "            nb_channels = self.R_stack_sizes[l]\n",
    "            self.layers_a.add_module(str('Ahat%i' %l),nn.Conv2d(in_channels  = nb_channels, \n",
    "                                                            out_channels = self.stack_sizes[l],\n",
    "                                                            stride       = (1, 1),\n",
    "                                                            kernel_size  = self.Ahat_filt_sizes[l],\n",
    "                                                            padding      = (-1 + self.Ahat_filt_sizes[l]) // 2))\n",
    "\n",
    "            if l == nb_layers-1:\n",
    "                nb_channels = 2 * self.stack_sizes[l] + self.R_stack_sizes[l]\n",
    "            else:\n",
    "                nb_channels = 2 * self.stack_sizes[l+1] + self.R_stack_sizes[l]\n",
    "            self.layers_d.add_module(str('conv%i' % l),\n",
    "                          nn.Conv2d(in_channels  = nb_channels, \n",
    "                                     out_channels = self.stack_sizes[l],\n",
    "                                     stride       = (1, 1),\n",
    "                                     kernel_size  = self.Ahat_filt_sizes[l],\n",
    "                                     padding      = (-1 + self.Ahat_filt_sizes[l]) // 2))\n",
    "            \n",
    "            if l < nb_layers - 1:\n",
    "                nb_channels = 2 * self.stack_sizes[l] + self.R_stack_sizes[l]\n",
    "                self.layers_u.add_module(str('conv%i' % l),nn.Conv2d(in_channels  = nb_channels,\n",
    "                                                    out_channels = self.R_stack_sizes[l],\n",
    "                                                    kernel_size  = self.R_filt_sizes[l],\n",
    "                                                    stride       = (1, 1),\n",
    "                                                    padding      = (-1 + self.R_filt_sizes[l]) // 2)\n",
    "                                                    )\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "\n",
    "    def get_initial_states(self, input_shape):\n",
    "        '''\n",
    "        input_shape is like: (batch_size, timeSteps, Height, Width, 3)\n",
    "                         or: (batch_size, timeSteps, 3, Height, Width)\n",
    "        '''\n",
    "        init_height = input_shape[self.row_axis]     # equal to `init_nb_rows` in original version\n",
    "        init_width  = input_shape[self.column_axis]     # equal to `init_nb_cols` in original version\n",
    "\n",
    "        base_initial_state = np.zeros(input_shape)\n",
    "        non_channel_axis = -1 if self.data_format == 'channels_first' else -2\n",
    "        for _ in range(2):\n",
    "            base_initial_state = np.sum(base_initial_state, axis = non_channel_axis)\n",
    "        base_initial_state = np.sum(base_initial_state, axis = 1)   # (batch_size, 3)\n",
    "\n",
    "        initial_states = []\n",
    "        states_to_pass = ['R', 'E']    # R is `representation`, c is Cell state in GRU, E is `error`.\n",
    "        layerNum_to_pass = {sta: self.num_layers for sta in states_to_pass}\n",
    "        if self.extrap_start_time is not None:\n",
    "            states_to_pass.append('Ahat')   # pass prediction in states so can use as actual for t+1 when extrapolating\n",
    "            layerNum_to_pass['Ahat'] = 1\n",
    "\n",
    "        for sta in states_to_pass:\n",
    "            for lay in range(layerNum_to_pass[sta]):\n",
    "                downSample_factor = 2 ** lay            \n",
    "                row = init_height // downSample_factor\n",
    "                col = init_width  // downSample_factor\n",
    "                if sta in ['R']:\n",
    "                    stack_size = self.R_stack_sizes[lay]\n",
    "                elif sta == 'E':\n",
    "                    stack_size = self.stack_sizes[lay] * 2\n",
    "                elif sta == 'Ahat':\n",
    "                    stack_size = self.stack_sizes[lay]\n",
    "                output_size = stack_size * row * col    # flattened size\n",
    "                reducer = np.zeros((input_shape[self.channel_axis], output_size))   # (3, output_size)\n",
    "                initial_state = np.dot(base_initial_state, reducer)                 # (batch_size, output_size)\n",
    "\n",
    "                if self.data_format == 'channels_first':\n",
    "                    output_shape = (-1, stack_size, row, col)\n",
    "                else:\n",
    "                    output_shape = (-1, row, col, stack_size)\n",
    "                # initial_state = torch.from_numpy(np.reshape(initial_state, output_shape)).float().to(device)\n",
    "                initial_state = Variable(torch.from_numpy(np.reshape(initial_state, output_shape)).float().to(self.device), requires_grad = True)\n",
    "                initial_states += [initial_state]\n",
    "\n",
    "        if self.extrap_start_time is not None:\n",
    "            initial_states += [Variable(torch.IntTensor(1).zero_().to(self.device))]   # the last state will correspond to the current timestep\n",
    "\n",
    "        return initial_states\n",
    "\n",
    "\n",
    "    def forward(self, a, states):\n",
    "        h_tm1 = states[:self.nb_layers]\n",
    "        e_tm1 = states[self.nb_layers:2*self.nb_layers]\n",
    "        \n",
    "        if self.extrap_start_time is not None:\n",
    "            t = states[-1]\n",
    "        \n",
    "        a0 = a[:]\n",
    "\n",
    "        h = []\n",
    "        e = []\n",
    "        ahat_list=[]\n",
    "\n",
    "        ########## Update R units starting from the top ##########\n",
    "        for l in reversed(range(self.nb_layers)):\n",
    "            if l == self.nb_layers - 1:\n",
    "                inputs = [h_tm1[l], e_tm1[l]]\n",
    "            else: \n",
    "                inputs = [h_tm1[l], ed]\n",
    "                \n",
    "            inputs = torch.cat(inputs, dim=self.channel_axis)\n",
    "            if not isinstance(inputs, Variable):\n",
    "                inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "            _h = hard_sigmoid(self.layers_d['conv{:d}'.format(l)](inputs))\n",
    "#             z = hard_sigmoid(self.conv_layers['zd'][l](inputs))\n",
    "#             o = hard_sigmoid(self.conv_layers['od'][l](inputs))\n",
    "#             _o = torch.tanh(o * self.conv_layers['hd'][l](inputs))\n",
    "#             _h = (1-z) * h_tm1[l] + z * _o\n",
    "            h.insert(0, _h)\n",
    "\n",
    "            ahat = self.conv_layers['ahat'][l](h[0])\n",
    "            if l == 0:\n",
    "                ahat[ahat > self.pixel_max] = self.pixel_max        # passed through a saturating non-linearity set at the maximum pixel value\n",
    "                frame_prediction = ahat\n",
    "            ahat_list.insert(0,ahat)\n",
    "            \n",
    "            if l > 0:\n",
    "                a = self.pool(h_tm1[l-1])\n",
    "            else:\n",
    "                if self.extrap_start_time is not None:\n",
    "                    if t >= self.t_extrap:\n",
    "                        a = ahat\n",
    "                    else:\n",
    "                        a = a0\n",
    "                else:\n",
    "                    a = a0\n",
    "            \n",
    "            ########## compute errors ##########\n",
    "            e_up = F.relu(ahat - a)\n",
    "            e_down = F.relu(a - ahat)\n",
    "\n",
    "            e.insert(0, torch.cat((e_up, e_down), dim=self.channel_axis))\n",
    "\n",
    "            if l > 0:\n",
    "                ed = self.upsample(e[0])\n",
    "            \n",
    "            if self.output_layer_num == l:\n",
    "                if self.output_layer_type == 'Ad':\n",
    "                    output = a\n",
    "                elif self.output_layer_type == 'Ahatd':\n",
    "                    output = ahat\n",
    "                elif self.output_layer_type == 'Hd':\n",
    "                    output = h[l]\n",
    "                elif self.output_layer_type == 'Ed':\n",
    "                    output = e[l]\n",
    "\n",
    "        ########## Update feedforward path starting from the bottom ##########\n",
    "        for l in range(self.nb_layers):\n",
    "            if l == 0:\n",
    "                pass\n",
    "            else:\n",
    "                a = self.pool(h[l-1])\n",
    "                ahat = ahat_list[l]\n",
    "                e_up = F.relu(ahat - a)\n",
    "                e_down = F.relu(a - ahat)\n",
    "                e[l] = torch.cat((e_up, e_down), axis = self.channel_axis)\n",
    "\n",
    "            if l < self.nb_layers - 1:\n",
    "                inputs = [h[l], e[l]]\n",
    "                inputs = torch.cat(inputs, dim=self.channel_axis)\n",
    "                if not isinstance(inputs, Variable):\n",
    "                    inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "                z = hard_sigmoid(self.conv_layers['zu'][l](inputs))\n",
    "                o = hard_sigmoid(self.conv_layers['ou'][l](inputs))\n",
    "                _o = torch.tanh(o * self.conv_layers['hu'][l](inputs))\n",
    "                _h = (1-z) * h[l] + z * _o\n",
    "                h[l] = _h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "derived-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PreCNet(params['stack_sizes'], params['R_stack_sizes'],\n",
    "                    params['Ahat_filt_sizes'], params['R_filt_sizes'], \n",
    "                    pixel_max=1, output_mode=params['output_mode'], return_sequences=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fallen-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = model.get_initial_states(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "traditional-bidding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 128, 16])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "amber-multiple",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 12, 3, 3], expected input[8, 16, 64, 8] to have 12 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-2a7f9821b23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 12, 3, 3], expected input[8, 16, 64, 8] to have 12 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "model.layers_d['conv0'](initial_state[1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-intensity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "personal-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModuleList(\n",
       "   (conv0): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (conv1): Conv2d(24, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       " ),\n",
       " ModuleList(\n",
       "   (conv0): Conv2d(6, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       " ),\n",
       " ModuleList(\n",
       "   (Ahat0): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (Ahat1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       " ))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_d,layers_u,layers_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "decimal-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 7, 1, 128, 16])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-classification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a619ce00633305a8885f99ed52cbc28eb145ea99de4cc5b4ecbe1c4930e283fe"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
