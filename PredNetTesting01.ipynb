{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "special-regard",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 10:34:37.165444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "# import umap\n",
    "# import umap.plot \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import sys\n",
    "# sys.path.insert(0, os.path.join(os.path.expanduser('~/Research/MyRepos/'),'SensoryMotorPred'))\n",
    "# from datasets import WCDataset, WCShotgunDataset, WC3dDataset\n",
    "import PreCNet as PN\n",
    "from datasets import AudioDataset\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({'font.size':         24,\n",
    "                     'axes.linewidth':    3,\n",
    "                     'xtick.major.size':  5,\n",
    "                     'xtick.major.width': 2,\n",
    "                     'ytick.major.size':  5,\n",
    "                     'ytick.major.width': 2,\n",
    "                     'axes.spines.right': False,\n",
    "                     'axes.spines.top':   False,\n",
    "                     'font.sans-serif':  \"Arial\",\n",
    "                     'font.family':      \"sans-serif\",\n",
    "                    })\n",
    "\n",
    "########## Checks if path exists, if not then creates directory ##########\n",
    "def check_path(basepath, path):\n",
    "    if path in basepath:\n",
    "        return basepath\n",
    "    elif not os.path.exists(os.path.join(basepath, path)):\n",
    "        os.makedirs(os.path.join(basepath, path))\n",
    "        print('Added Directory:'+ os.path.join(basepath, path))\n",
    "        return os.path.join(basepath, path)\n",
    "    else:\n",
    "        return os.path.join(basepath, path)\n",
    "\n",
    "rootdir = os.path.expanduser('~/Desktop/Research/Murray/git/PredAudio/')\n",
    "\n",
    "# Set up partial functions for directory managing\n",
    "join = partial(os.path.join,rootdir)\n",
    "checkDir = partial(check_path,rootdir)\n",
    "FigurePath = checkDir('Figures')\n",
    "\n",
    "savefigs=False\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-tiffany",
   "metadata": {},
   "source": [
    "# Test Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Matt/Desktop/Research/Murray/git/PredAudio/results/Tau01/NotStateful/PreCNetGRU_T007_N07_E1500_Tau01_Visual_Netparams.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Ahat_filt_sizes': [3, 3, 3, 3],\n 'BatchSize': 64,\n 'FiltNum': 4,\n 'ImageSize': 2048,\n 'KSize': 3,\n 'LogDir': '/home/seuss/Research/PredAudio/results/Tau01/NotStateful/Logs/Trial_07',\n 'Nepochs': 1500,\n 'Overlap': 8,\n 'R_filt_sizes': [3, 3, 3, 3],\n 'R_stack_sizes': [4, 16, 64, 256],\n 'Save_Grad': 0,\n 'Stateful': 0,\n 'Tau': 1,\n 'Test_paths': '/Users/Matt/Desktop/Research/Murray/git/PredAudio/Specs_test.npy',\n 'TimeSize': 7,\n 'Train_paths': '/Users/Matt/Desktop/Research/Murray/git/PredAudio/Specs_train.npy',\n 'Trial': 7,\n 'WindSize': 16,\n 'data_format': 'channels_first',\n 'filename': 'PreCNetGRU_T007_N07_E1500_Tau01_Visual',\n 'height': 128,\n 'input_shape': [64, 7, 128, 16, 1],\n 'layer_loss_weightsMode': 'L_0',\n 'log_freq': 10,\n 'lr': 0.005,\n 'output_mode': 'error',\n 'save_path': '/Users/Matt/Desktop/Research/Murray/git/PredAudio/results/Tau01/NotStateful',\n 'stack_sizes': [1, 4, 16, 64],\n 'use_motor': 0,\n 'width': 16}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Trial    = 7\n",
    "TimeSize = 7\n",
    "\n",
    "output_mode = 'error'\n",
    "save_path   = os.path.expanduser('~/Desktop/Research/Murray/git/PredAudio/results/Tau01/NotStateful')\n",
    "fileList    = sorted(glob.glob(os.path.join(save_path, 'PreCNetGRU_T{:03d}_N{:02d}_E*_Tau01_Visual_Netparams.json'.format(TimeSize,Trial))))\n",
    "filename    = fileList[0]\n",
    "Netpath = sorted(glob.glob(os.path.join(save_path,'PreCNetGRU_T{:03d}_N{:02d}_E*_Tau01_Visual*.pt'.format(TimeSize,Trial))))\n",
    "print(filename)\n",
    "if len(fileList)>0 and os.path.exists(fileList[0]):\n",
    "    with open(fileList[0], 'r') as fp:\n",
    "        netparams = json.load(fp)\n",
    "\n",
    "netparams['Train_paths'] = os.path.expanduser('~/Desktop/Research/Murray/data/PredAudioData/soundFiles/Specs_train.npy')\n",
    "netparams['Test_paths'] = os.path.expanduser('~/Desktop/Research/Murray/data/PredAudioData/soundFiles/Specs_test.npy')\n",
    "netparams['save_path'] = save_path\n",
    "netparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparable-reynolds",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Network\n"
     ]
    }
   ],
   "source": [
    "if netparams['data_format'] == 'channels_first':\n",
    "    input_shape = (netparams['BatchSize'], netparams['TimeSize'], 1, netparams['height'], netparams['width'])\n",
    "else:\n",
    "    input_shape = (netparams['BatchSize'], netparams['TimeSize'], netparams['height'], netparams['width'], 1)\n",
    "\n",
    "precnet,netparams = PN.getNetwork(netparams)\n",
    "optimizer = torch.optim.Adam(precnet.parameters(), lr = netparams['lr'])\n",
    "params = torch.load(Netpath[0], map_location=torch.device('cpu'))\n",
    "precnet.load_state_dict(params['Model_state_dict'])\n",
    "precnet.to(device)\n",
    "print('Loaded Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-deficit",
   "metadata": {},
   "source": [
    "netparams['WindSize'] = 16\n",
    "netparams['Overlap'] = netparams['WindSize']\n",
    "netparams['TimeSize'] = 64//netparams['WindSize']\n",
    "netparams['WindSize'],netparams['Overlap'],netparams['TimeSize'],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-anaheim",
   "metadata": {},
   "source": [
    "netparams['Overlap'] = netparams['WindSize']//2\n",
    "netparams['Test_paths'] = '/home/seuss/Research/PredAudio/Specs_test_mfcc.npy'\n",
    "netparams['Train_paths'] = '/home/seuss/Research/PredAudio/Specs_train_mfcc.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-treatment",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m sum_trainLoss_in_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     22\u001B[0m min_trainLoss_in_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 24\u001B[0m initial_states \u001B[38;5;241m=\u001B[39m precnet\u001B[38;5;241m.\u001B[39mget_initial_states(input_shape)\n\u001B[1;32m     25\u001B[0m states \u001B[38;5;241m=\u001B[39m initial_states\n",
      "File \u001B[0;32m~/Desktop/Research/Murray/git/PredAudio/PreCNet.py:258\u001B[0m, in \u001B[0;36mPreCNet.get_initial_states\u001B[0;34m(self, input_shape)\u001B[0m\n\u001B[1;32m    256\u001B[0m             output_shape \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, row, col, stack_size)\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;66;03m# initial_state = torch.from_numpy(np.reshape(initial_state, output_shape)).float().to(device)\u001B[39;00m\n\u001B[0;32m--> 258\u001B[0m         initial_state \u001B[38;5;241m=\u001B[39m Variable(torch\u001B[38;5;241m.\u001B[39mfrom_numpy(np\u001B[38;5;241m.\u001B[39mreshape(initial_state, output_shape))\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), requires_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    259\u001B[0m         initial_states \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [initial_state]\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextrap_start_time \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tr_loss = 0.0\n",
    "sum_trainLoss_in_epoch = 0.0\n",
    "min_trainLoss_in_epoch = float('inf')\n",
    "# tonespath = os.path.join(rootdir,'Specs_puretones_logscale.npy')\n",
    "tonespath = os.path.join(rootdir,'Specs_mfcc.npy')\n",
    "\n",
    "########## Create Datasets and DataLoaders ##########\n",
    "Dataset_Train = AudioDataset(netparams['Train_paths'],netparams['WindSize'],netparams['Overlap'])\n",
    "Dataset_Test = AudioDataset(netparams['Test_paths'],netparams['WindSize'],netparams['Overlap'])\n",
    "Dataset_Pure = AudioDataset(tonespath,netparams['WindSize'],netparams['Overlap'])\n",
    "num_workers = multiprocessing.cpu_count()//2\n",
    "DataLoader_Train = DataLoader(Dataset_Train, batch_size=netparams['BatchSize'], shuffle=False, drop_last=True, num_workers=num_workers, pin_memory=True)\n",
    "DataLoader_Test = DataLoader(Dataset_Test, batch_size=netparams['BatchSize'], shuffle=False, drop_last=True, num_workers=num_workers, pin_memory=True)\n",
    "DataLoader_Pure = DataLoader(Dataset_Pure, batch_size=netparams['BatchSize'], shuffle=False, drop_last=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "lr_maker  = lr_scheduler.StepLR(optimizer = optimizer, step_size = 100, gamma = 0.1)  # decay the lr every 50 epochs by a factor of 0.1\n",
    "totsteps = 0\n",
    "\n",
    "Epoch = 0\n",
    "tr_loss = 0.0\n",
    "sum_trainLoss_in_epoch = 0.0\n",
    "min_trainLoss_in_epoch = float('inf')\n",
    "\n",
    "initial_states = precnet.get_initial_states(input_shape)\n",
    "states = initial_states\n",
    "# step = 0\n",
    "\n",
    "# precnet, optimizer = amp.initialize(precnet, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-gilbert",
   "metadata": {},
   "source": [
    "## Grab States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = precnet.get_initial_states(input_shape)\n",
    "states = initial_states\n",
    "h_tr = {'h{:d}'.format(n):[] for n in range(len(netparams['R_stack_sizes']))}\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(DataLoader_Train):\n",
    "        output, states, frame_pred = precnet.grab_states(batch.to(device), states, grab_frame=True)\n",
    "        for n, state in enumerate(states[:len(netparams['R_stack_sizes'])]):\n",
    "            h_tr['h{:d}'.format(n)].append(output['h{:d}'.format(n)])\n",
    "        initial_states = precnet.get_initial_states(input_shape)\n",
    "        states = initial_states\n",
    "for n in range(len(netparams['R_stack_sizes'])):\n",
    "     h_tr['h{:d}'.format(n)] = np.stack(h_tr['h{:d}'.format(n)]).transpose(0,2,1,3,4,5).reshape((-1,netparams['TimeSize'],) + h_tr['h{:d}'.format(n)][0].shape[-3:])\n",
    "        \n",
    "# initial_states = precnet.get_initial_states(input_shape)\n",
    "# states = initial_states\n",
    "# h_te = {'h{:d}'.format(n):[] for n in range(len(netparams['R_stack_sizes']))}\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(DataLoader_Test):\n",
    "#         output, states, frame_pred = precnet.grab_states(batch.to(device), states, grab_frame=True)\n",
    "#         for n, state in enumerate(states[:len(netparams['R_stack_sizes'])]):\n",
    "#             h_te['h{:d}'.format(n)].append(output['h{:d}'.format(n)])\n",
    "#         initial_states = precnet.get_initial_states(input_shape)\n",
    "#         states = initial_states\n",
    "# for n in range(len(netparams['R_stack_sizes'])):\n",
    "#      h_te['h{:d}'.format(n)] = np.stack(h_te['h{:d}'.format(n)]).transpose(0,2,1,3,4,5).reshape((-1,netparams['TimeSize'],) + h_te['h{:d}'.format(n)][0].shape[-3:])\n",
    "\n",
    "        \n",
    "# initial_states = precnet.get_initial_states(input_shape)\n",
    "# states = initial_states\n",
    "# h_pure = {'h{:d}'.format(n):[] for n in range(len(netparams['R_stack_sizes']))}\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(DataLoader_Pure):\n",
    "#         output, states, frame_pred = precnet.grab_states(batch.to(device), states, grab_frame=True)\n",
    "#         for n, state in enumerate(states[:len(netparams['R_stack_sizes'])]):\n",
    "#             h_pure['h{:d}'.format(n)].append(output['h{:d}'.format(n)])\n",
    "#         initial_states = precnet.get_initial_states(input_shape)\n",
    "#         states = initial_states\n",
    "# for n in range(len(netparams['R_stack_sizes'])):\n",
    "#      h_pure['h{:d}'.format(n)] = np.stack(h_pure['h{:d}'.format(n)]).transpose(0,2,1,3,4,5).reshape((-1,netparams['TimeSize'],) + h_pure['h{:d}'.format(n)][0].shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "romance-crazy",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Matt/Desktop/Research/Murray/git/PredAudio/labels_puretones_logscale.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m labels_test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(rootdir,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      8\u001B[0m labels_test \u001B[38;5;241m=\u001B[39m labels_test\u001B[38;5;241m.\u001B[39miloc[:tot_test]\n\u001B[0;32m----> 9\u001B[0m labels_pure_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(rootdir,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels_puretones_logscale.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     10\u001B[0m labels_pure_df \u001B[38;5;241m=\u001B[39m labels_pure_df\u001B[38;5;241m.\u001B[39miloc[:tot_pure]\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    945\u001B[0m )\n\u001B[1;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1706\u001B[0m     f,\n\u001B[1;32m   1707\u001B[0m     mode,\n\u001B[1;32m   1708\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1709\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1710\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1711\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1712\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1713\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1714\u001B[0m )\n\u001B[1;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/pandas/io/common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    864\u001B[0m             handle,\n\u001B[1;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    869\u001B[0m         )\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/Matt/Desktop/Research/Murray/git/PredAudio/labels_puretones_logscale.csv'"
     ]
    }
   ],
   "source": [
    "tot_train = len(DataLoader_Train)*netparams['BatchSize']\n",
    "tot_test = len(DataLoader_Test)*netparams['BatchSize']\n",
    "tot_pure = len(DataLoader_Pure)*netparams['BatchSize']\n",
    "\n",
    "labels_train = pd.read_csv(os.path.join(rootdir,'labels_train.csv'))\n",
    "labels_train = labels_train.iloc[:tot_train]\n",
    "labels_test = pd.read_csv(os.path.join(rootdir,'labels_test.csv'))\n",
    "labels_test = labels_test.iloc[:tot_test]\n",
    "labels_pure_df = pd.read_csv(os.path.join(rootdir,'labels_puretones_logscale.csv'))\n",
    "labels_pure_df = labels_pure_df.iloc[:tot_pure]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output, states, frame_pred = precnet(batch.to(device), states, grab_frame=True)\n",
    "FM_Pred = np.stack(frame_pred).transpose(1,0,2).reshape(netparams['BatchSize'], netparams['TimeSize'], netparams['height'], netparams['width'])\n",
    "\n",
    "FM_Actual = batch.squeeze().numpy()\n",
    "GT = np.squeeze(batch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmGT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-healing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(FigurePath,'GT_Pred.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "7,12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-romania",
   "metadata": {},
   "source": [
    "add labels to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instant-laser",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mrcParams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfigure.facecolor\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhite\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m savefigs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m GT \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqueeze(batch\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m      6\u001B[0m fmGT \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(GT[n])\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      7\u001B[0m fmP \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(FM_Pred[n])\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "n=7\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "savefigs=False\n",
    "GT = np.squeeze(batch.detach().numpy())\n",
    "fmGT = torch.FloatTensor(GT[n]).unsqueeze(1)\n",
    "fmP = torch.FloatTensor(FM_Pred[n]).unsqueeze(1)\n",
    "GT_Grid = torchvision.utils.make_grid(fmGT[:,:,:,8:],nrow=netparams['TimeSize'])\n",
    "P_Grid = torchvision.utils.make_grid(fmP[:,:,:,8:],nrow=netparams['TimeSize'])\n",
    "\n",
    "\n",
    "fig1, axs = plt.subplots(2, 1, figsize = (20,10))\n",
    "axs[0].imshow(GT_Grid[0], aspect='auto',  origin='lower') #cmap='gray'\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Ground Truth, Digit: {:d}'.format(labels_train['digits'][n]))\n",
    "axs[1].imshow(P_Grid[0],  aspect='auto',  origin='lower') #cmap='gray'\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "if savefigs:\n",
    "    fig1.savefig(os.path.join(FigurePath,'GT_Pred_{:d}.pdf'.format(n)), facecolor=fig1.get_facecolor(), transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in h_tr.keys():\n",
    "    print(h_tr[key].shape, h_tr[key].shape[-1]*h_tr[key].shape[-2]*h_tr[key].shape[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "8*128*32, 64*64*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-arbor",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-guide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "personalized-huntington",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m      3\u001B[0m pcaL \u001B[38;5;241m=\u001B[39m PCA(n_components\u001B[38;5;241m=\u001B[39mN_comp)\n\u001B[0;32m----> 4\u001B[0m zcomps \u001B[38;5;241m=\u001B[39m pcaL\u001B[38;5;241m.\u001B[39mfit_transform(h_tr[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;132;01m{:d}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(layer)][:,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mreshape(tot_train,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m      6\u001B[0m exp_var_cumul \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcumsum(pcaL\u001B[38;5;241m.\u001B[39mexplained_variance_ratio_)\n\u001B[1;32m      7\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39marea(x\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, exp_var_cumul\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m),\n\u001B[1;32m      8\u001B[0m         y\u001B[38;5;241m=\u001B[39mexp_var_cumul,\n\u001B[1;32m      9\u001B[0m         labels\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# Components\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExplained Variance\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     10\u001B[0m         title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mncomps 95\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(exp_var_cumul[exp_var_cumul\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m.95\u001B[39m]),N_comp)\n\u001B[1;32m     11\u001B[0m         )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'h_tr' is not defined"
     ]
    }
   ],
   "source": [
    "N_comp=200\n",
    "layer = 2\n",
    "pcaL = PCA(n_components=N_comp)\n",
    "zcomps = pcaL.fit_transform(h_tr['h{:d}'.format(layer)][:,-1].reshape(tot_train,-1))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pcaL.explained_variance_ratio_)\n",
    "fig = px.area(x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "        y=exp_var_cumul,\n",
    "        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "        title='ncomps 95%: {}/{}'.format(len(exp_var_cumul[exp_var_cumul<.95]),N_comp)\n",
    "        )\n",
    "fig.show()\n",
    "\n",
    "N_comp=200\n",
    "layer = 2\n",
    "pcaL = PCA(n_components=N_comp)\n",
    "zcomps = pcaL.fit_transform(h_te['h{:d}'.format(layer)][:,-1].reshape(tot_train,-1))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pcaL.explained_variance_ratio_)\n",
    "fig = px.area(x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "        y=exp_var_cumul,\n",
    "        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "        title='ncomps 95%: {}/{}'.format(len(exp_var_cumul[exp_var_cumul<.95]),N_comp)\n",
    "        )\n",
    "fig.show()\n",
    "\n",
    "N_comp=200\n",
    "layer = 2\n",
    "pcaL = PCA(n_components=N_comp)\n",
    "zcomps = pcaL.fit_transform(h_pure['h{:d}'.format(layer)][:,-1].reshape(tot_pure,-1))\n",
    "\n",
    "exp_var_cumul = np.cumsum(pcaL.explained_variance_ratio_)\n",
    "fig = px.area(x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "        y=exp_var_cumul,\n",
    "        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "        title='ncomps 95%: {}/{}'.format(len(exp_var_cumul[exp_var_cumul<.95]),N_comp)\n",
    "        )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "zcomps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCA_States(ht,netparams,labels_df,tot_trials,FigPath,N_comp=30,savefigs=False,Type='Train'):\n",
    "    ztot = []\n",
    "    for layer in trange(len(netparams['R_stack_sizes'])):\n",
    "        pcaL = PCA(n_components=N_comp)\n",
    "        zcomps = pcaL.fit_transform(ht['h{:d}'.format(layer)][:,:].reshape(tot_trials,-1))\n",
    "        ztot.append(zcomps)\n",
    "        exp_var_cumul = np.cumsum(pcaL.explained_variance_ratio_)\n",
    "        fig = px.area(x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "                y=exp_var_cumul,\n",
    "                labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "                title='ncomps 95%: {}/{}'.format(len(exp_var_cumul[exp_var_cumul<.95]),N_comp)\n",
    "                )\n",
    "    #     fig.show()\n",
    "        if savefigs:\n",
    "\n",
    "            labels = {\n",
    "                str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "                for i, var in enumerate(pcaL.explained_variance_ratio_ * 100)\n",
    "            }\n",
    "\n",
    "            fig2 = px.scatter_matrix(\n",
    "                zcomps,\n",
    "                labels=labels,\n",
    "                dimensions=range(2),\n",
    "                color=labels_df['digits'],\n",
    "\n",
    "            )\n",
    "            fig2.update_traces(diagonal_visible=False)\n",
    "            fig2.update_layout(autosize=False,\n",
    "                              coloraxis_colorbar=dict(title='Time'),\n",
    "                              width=1000,\n",
    "                              height=1000,\n",
    "                              )\n",
    "            fig.write_image(os.path.join(check_path(FigPath,Type),'ExplainedVar_L{:d}_Trial{}.png'.format(layer,Trial)))\n",
    "            fig2.write_image(os.path.join(check_path(FigPath,Type),'PCAMat_L{:d}_Trial{}.png'.format(layer,Trial)))\n",
    "    return ztot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_comp=9\n",
    "FigPath = check_path(FigurePath,'{}'.format(netparams['filename']))\n",
    "savefigs = False\n",
    "ztot_tr = PCA_States(h_tr,netparams,labels_train,tot_train,FigPath,N_comp=N_comp,savefigs=True,Type='Train_LogScale')\n",
    "ztot_te = PCA_States(h_te,netparams,labels_test,tot_test,FigPath,N_comp=N_comp,savefigs=True,Type='Test_LogScale')\n",
    "ztot_pure = PCA_States(h_pure,netparams,labels_pure_df,tot_pure,FigPath,N_comp=N_comp,savefigs=True,Type='Pure_LogScale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=0\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pcaL.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    ztot_tr[layer],\n",
    "    labels=labels_test,\n",
    "    dimensions=range(4),\n",
    "    color=labels_train['digits'],\n",
    "   \n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.update_layout(autosize=False,\n",
    "                  coloraxis_colorbar=dict(title='Time'),\n",
    "                  width=1000,\n",
    "                  height=1000,\n",
    "                  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=3\n",
    "fig = go.Figure(data=[go.Scatter3d(x=ztot_tr[layer][:,0], y=ztot_tr[layer][:,1], z=ztot_tr[layer][:,2],\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(\n",
    "                                                size=5,\n",
    "                                                color= pd.factorize(labels_train['speaker'])[0], # labels_train['digits'], # pd.factorize(labels_test['speaker'])[0], #\n",
    "                                                colorbar=dict(title='Digit'),\n",
    "                                                opacity=0.8),\n",
    "#                                    line=dict(\n",
    "#                                              color=np.arange(zcomps[t:t+dt,0].shape[0])/30,\n",
    "#                                              width=2)\n",
    "                                   )])\n",
    "fig.update_layout(autosize=False,\n",
    "                  coloraxis_colorbar=dict(title='Time (s)'),\n",
    "                  width=500,\n",
    "                  height=500,\n",
    "                  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-upper",
   "metadata": {},
   "source": [
    "## Linear Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 3\n",
    "data_type = 'PCA'\n",
    "def train_classifer(data,ht,labels,netparams,tot_trials,data_type=data_type):\n",
    "    scores = []\n",
    "    for layer in trange(len(netparams['R_stack_sizes'])):\n",
    "        if data_type == 'PCA':\n",
    "            X = data[layer]\n",
    "        else:\n",
    "            X = ht['h{:d}'.format(layer)][:,1].reshape(tot_trials,-1) #  \n",
    "\n",
    "        clf = make_pipeline(StandardScaler(),\n",
    "#                             LinearSVC(random_state=0, tol=1e-5, max_iter=10000),)\n",
    "                            LogisticRegression(random_state=0, tol=1e-5, max_iter=10000,n_jobs=-1)) # \n",
    "\n",
    "        # Split data into 50% train and 50% test subsets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, shuffle=False, random_state=1)\n",
    "\n",
    "        # Learn the digits on the train subset\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        predicted = clf.predict(X_test)\n",
    "        scores.append(metrics.accuracy_score(y_test, predicted))\n",
    "        print('Layer: {}, Score: {}'.format(layer, scores[layer]))\n",
    "#         print(f\"Classification report for classifier layer{layer} {clf}:\\n\"\n",
    "#               f\"{metrics.classification_report(y_test, predicted)}\\n\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-certificate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_tr = train_classifer(ztot_tr,h_tr,labels_train['digits'].to_numpy(),netparams,tot_train,data_type='PCA')\n",
    "scores_te = train_classifer(ztot_te,h_te,labels_test['digits'].to_numpy(),netparams,tot_test,data_type='PCA')\n",
    "scores_pure = train_classifer(ztot_pure,h_pure,labels_pure_df['digits'].to_numpy(),netparams,tot_pure,data_type='PCA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztot_tr2 = np.hstack(ztot_tr)\n",
    "ztot_te2 = np.hstack(ztot_te)\n",
    "ztot_pure2 = np.hstack(ztot_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 3\n",
    "data_type = 'PCA'\n",
    "def train_classifer_compiled(X,ht,labels,netparams,tot_trials,data_type=data_type):\n",
    "    scores = []\n",
    "    clf = make_pipeline(StandardScaler(),\n",
    "#                             LinearSVC(random_state=0, tol=1e-5, max_iter=10000),)\n",
    "                        LogisticRegression(random_state=0, tol=1e-5, max_iter=10000,n_jobs=-1)) # \n",
    "\n",
    "    # Split data into 50% train and 50% test subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, shuffle=False, random_state=1)\n",
    "\n",
    "    # Learn the digits on the train subset\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, predicted))\n",
    "    print('Layer: all, Score: {}'.format(scores))\n",
    "#         print(f\"Classification report for classifier layer{layer} {clf}:\\n\"\n",
    "#               f\"{metrics.classification_report(y_test, predicted)}\\n\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tr2 = train_classifer_compiled(ztot_tr2,h_tr,labels_train['digits'].to_numpy(),netparams,tot_train,data_type='PCA')\n",
    "scores_te2 = train_classifer_compiled(ztot_te2,h_te,labels_test['digits'].to_numpy(),netparams,tot_test,data_type='PCA')\n",
    "scores_pure2 = train_classifer_compiled(ztot_pure2,h_pure,labels_pure_df['digits'].to_numpy(),netparams,tot_pure,data_type='PCA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tr = np.array(scores_tr)\n",
    "scores_te = np.array(scores_te)\n",
    "scores_pure = np.array(scores_pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-queue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-newman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-freeze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-judge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enabling-marriage",
   "metadata": {},
   "source": [
    "# Create Grant Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_comp=7\n",
    "FigPath = check_path(FigurePath,'{}'.format(netparams['filename']))\n",
    "\n",
    "for N_comp in range(1,20):\n",
    "    ztot_tr = PCA_States(h_tr,netparams,labels_train,tot_train,FigPath,N_comp=N_comp,savefigs=False,Type='Train_LogScale')\n",
    "    ztot_te = PCA_States(h_te,netparams,labels_test,tot_test,FigPath,N_comp=N_comp,savefigs=False,Type='Test_LogScale')\n",
    "    ztot_pure = PCA_States(h_pure,netparams,labels_pure_df,tot_pure,FigPath,N_comp=N_comp,savefigs=False,Type='Pure_LogScale')\n",
    "\n",
    "    scores_tr = train_classifer(ztot_tr,h_tr,labels_train['digits'].to_numpy(),netparams,tot_train,data_type='PCA')\n",
    "    scores_te = train_classifer(ztot_te,h_te,labels_test['digits'].to_numpy(),netparams,tot_test,data_type='PCA')\n",
    "    scores_pure = train_classifer(ztot_pure,h_pure,labels_pure_df['digits'].to_numpy(),netparams,tot_pure,data_type='PCA')\n",
    "    scores_tr = np.array(scores_tr)\n",
    "    scores_te = np.array(scores_te)\n",
    "    scores_pure = np.array(scores_pure)\n",
    "\n",
    "    ztot_tr2 = np.hstack(ztot_tr)\n",
    "    ztot_te2 = np.hstack(ztot_te)\n",
    "    ztot_pure2 = np.hstack(ztot_pure)\n",
    "\n",
    "    scores_tr2 = train_classifer_compiled(ztot_tr2,h_tr,labels_train['digits'].to_numpy(),netparams,tot_train,data_type='PCA')\n",
    "    scores_te2 = train_classifer_compiled(ztot_te2,h_te,labels_test['digits'].to_numpy(),netparams,tot_test,data_type='PCA')\n",
    "    scores_pure2 = train_classifer_compiled(ztot_pure2,h_pure,labels_pure_df['digits'].to_numpy(),netparams,tot_pure,data_type='PCA')\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(12,5))\n",
    "    ax.bar([1,2],scores_te[[0,-1]],color='b',label='Spoken Digits')\n",
    "    ax.bar([4,5],scores_pure[[0,-1]],color='g', label='Pure Tones')\n",
    "    ax.axhline(y=1/len(np.unique(labels_train['digits'])),linestyle='--', color='red')\n",
    "    ax.bar(7,scores_te2,color='b',)\n",
    "    ax.bar(8,scores_pure2,color='g',)\n",
    "    ax.set_ylim(0,1.1)\n",
    "    ax.legend(bbox_to_anchor=(1.01, 1))\n",
    "    ax.set_xlabel('Layer #')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(np.arange(9))\n",
    "    ax.set_xticklabels(['','1','4','','1','4','','all','all'])\n",
    "    # ax.set_title('Labels=Digits')\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    fig.savefig(os.path.join(FigPath,'LogisticReg_Accuracy_{}_LogScale_PCA{:d}_withALL.pdf'.format(data_type,N_comp)))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(8,5))\n",
    "    ax.bar([1,2],scores_te[[0,-1]],color='b',label='Spoken Digits')\n",
    "    ax.bar([4,5],scores_pure[[0,-1]],color='g', label='Pure Tones')\n",
    "    ax.axhline(y=1/len(np.unique(labels_train['digits'])),linestyle='--', color='red')\n",
    "    # ax.set_ylim(0,1.1)\n",
    "    ax.legend()#,bbox_to_anchor=(1.01, 1))\n",
    "    ax.set_xlabel('Layer #')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(np.arange(6))\n",
    "    ax.set_xticklabels(['','1','4','','1','4'])\n",
    "    # ax.set_title('Labels=Digits')\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "    fig.savefig(os.path.join(FigPath,'LogisticReg_Accuracy_{}_LogScale_PCA{:d}.pdf'.format(data_type,N_comp)))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1,figsize=(10,5))\n",
    "    ax.plot(scores_tr, 'b.-',markersize=20, linewidth=3)\n",
    "    ax.plot(scores_te, 'r.-',markersize=20, linewidth=3)\n",
    "    ax.plot(scores_pure, 'g.-',markersize=20, linewidth=3)\n",
    "    ax.axhline(y=1/len(np.unique(labels_train['digits'])),linestyle='--', color='red')\n",
    "    ax.plot(4,scores_tr2, 'b.-',markersize=20, linewidth=3)\n",
    "    ax.plot(4,scores_te2, 'r.-',markersize=20, linewidth=3)\n",
    "    ax.plot(4,scores_pure2, 'g.-',markersize=20, linewidth=3)\n",
    "\n",
    "\n",
    "    ax.set_ylim(0,1.1)\n",
    "    ax.legend(['Train','Test','Pure Tones','Random'],bbox_to_anchor=(1.01, 1))\n",
    "    ax.set_xlabel('Layer #')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.set_xticklabels(['0','1','2','3','all'])\n",
    "    ax.set_title('Labels=Digits')\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    fig.savefig(os.path.join(FigPath,'LogisticReg_Accuracy_{}_LogScale_PCA{:d}_Scatter.pdf'.format(data_type,N_comp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-performance",
   "metadata": {},
   "source": [
    "Some difference in decoding accuracy across layers and is different for different type of stimuli. \n",
    "\n",
    "Pure tone input to trained PreCnet, want decoding in first layer to be the highest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-louis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztot_pure.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "disp = metrics.plot_confusion_matrix(clf, X_test, y_test)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-syndicate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(cv=2, random_state=0, n_jobs=-1).fit(X, y)\n",
    "# clf.predict(X[:2, :])\n",
    "\n",
    "# clf.predict_proba(X[:2, :]).shape\n",
    "\n",
    "# clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-lebanon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opened-springer",
   "metadata": {},
   "source": [
    "# Pure Tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as TV\n",
    "import torchaudio.transforms as AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or 'Spectrogram (db)')\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel('frame')\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-survivor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(1,4,10,base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sig=0\n",
    "f = 1\n",
    "phase = 0\n",
    "ntones = 10\n",
    "toneslist = [n*1000 for n in range(1,ntones+1)]\n",
    "ntrials = 64\n",
    "nsamples = 9000\n",
    "times = np.linspace(0,2,nsamples)\n",
    "tones = np.zeros((ntrials,ntones,nsamples))\n",
    "labels_pure = np.zeros((ntrials,ntones,1))\n",
    "amod = 5*np.random.rand(ntrials,ntones)\n",
    "\n",
    "freq, trials, = [], []\n",
    "for trial in trange(ntrials):\n",
    "    for n, f in enumerate(toneslist):\n",
    "        tones[trial,n] = amod[trial,n]*np.sin(f/(2*np.pi)*times + phase) + np.random.normal(loc=0,scale=sig,size=(nsamples))\n",
    "        freq.append(f)\n",
    "        trials.append(trial)\n",
    "labels_pure_df = pd.DataFrame({'digits':freq,'Trial':trials})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-mongolia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(times[:100],tones[0,9,:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 256\n",
    "win_length = None\n",
    "hop_length = 128\n",
    "\n",
    "# define transformation\n",
    "spects = AV.Spectrogram(\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "#     center=True,\n",
    "#     pad_mode=\"reflect\",\n",
    "#     power=2.0,\n",
    ")\n",
    "# Perform transformation\n",
    "spec = spects(torch.tensor(tones[0,-1],dtype=torch.float32))\n",
    "\n",
    "# print_stats(spec)\n",
    "plot_spectrogram(spec, title='torchaudio')\n",
    "print(spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf,nt = (128,71)\n",
    "resize = TV.Resize((nf,nt))\n",
    "tones2 = tones.reshape(-1,nsamples)\n",
    "data_spec1 = np.zeros((tones2.shape[0],1,nf,nt))\n",
    "for n in range(tones2.shape[0]):\n",
    "    mfcc = spects(torch.tensor(tones2[n],dtype=torch.float32)).numpy() #mfcc_transform\n",
    "#     mfcc = mfcc_transform(torch.tensor(data1[n],dtype=torch.float32)).numpy() #mfcc_transform\n",
    "    spec2 = resize(torch.Tensor(mfcc).unsqueeze(0))\n",
    "    data_spec1[n,:] = librosa.power_to_db(spec2.numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(rootdir,'Specs_puretones_logscale_nonoise.npy'),data_spec1)\n",
    "labels_pure_df.to_csv(os.path.join(rootdir,'labels_puretones_logscale_nonoise.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-mason",
   "metadata": {},
   "source": [
    "## Grab States for Pure Tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pure_df = pd.read_csv(os.path.join(rootdir,'labels_puretones.csv'))\n",
    "initial_states = precnet.get_initial_states(input_shape)\n",
    "states = initial_states\n",
    "ht_pure = {'h{:d}'.format(n):[] for n in range(len(netparams['R_stack_sizes']))}\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(DataLoader_Pure):\n",
    "        output, states, frame_pred = precnet.grab_states(batch.to(device), states, grab_frame=True)\n",
    "        for n, state in enumerate(states[:len(netparams['R_stack_sizes'])]):\n",
    "            ht_pure['h{:d}'.format(n)].append(output['h{:d}'.format(n)])\n",
    "        initial_states = precnet.get_initial_states(input_shape)\n",
    "        states = initial_states\n",
    "for n in range(len(netparams['R_stack_sizes'])):\n",
    "     ht_pure['h{:d}'.format(n)] = np.stack(ht_pure['h{:d}'.format(n)]).transpose(0,2,1,3,4,5).reshape((-1,netparams['TimeSize'],) + ht_pure['h{:d}'.format(n)][0].shape[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "FigPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_pure = len(Dataset_Pure)\n",
    "N_comp=200\n",
    "FigPath = check_path(FigurePath,'{}'.format(netparams['filename']))\n",
    "savefigs = True\n",
    "ztot = []\n",
    "for layer in trange(len(netparams['R_stack_sizes'])):\n",
    "    pcaL = PCA(n_components=N_comp)\n",
    "    zcomps = pcaL.fit_transform(ht['h{:d}'.format(layer)][:,:].reshape(tot_pure,-1))\n",
    "    ztot.append(zcomps)\n",
    "    exp_var_cumul = np.cumsum(pcaL.explained_variance_ratio_)\n",
    "    fig = px.area(x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "            y=exp_var_cumul,\n",
    "            labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n",
    "            title='ncomps 95%: {}/{}'.format(len(exp_var_cumul[exp_var_cumul<.95]),N_comp)\n",
    "            )\n",
    "#     fig.show()\n",
    "    \n",
    "    labels = {\n",
    "        str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "        for i, var in enumerate(pcaL.explained_variance_ratio_ * 100)\n",
    "    }\n",
    "\n",
    "    fig2 = px.scatter_matrix(\n",
    "        zcomps,\n",
    "        labels=labels,\n",
    "        dimensions=range(4),\n",
    "        color=labels_pure_df['Freq'],\n",
    "\n",
    "    )\n",
    "    fig2.update_traces(diagonal_visible=False)\n",
    "    fig2.update_layout(autosize=False,\n",
    "                      coloraxis_colorbar=dict(title='Time'),\n",
    "                      width=1000,\n",
    "                      height=1000,\n",
    "                      )\n",
    "    if savefigs:\n",
    "        fig.write_image(os.path.join(FigPath,'ExplainedVar_L{:d}_Trial{}_PureTones.png'.format(layer,Trial)))\n",
    "        fig2.write_image(os.path.join(FigPath,'PCAMat_L{:d}_Trial{}_PureTones.png'.format(layer,Trial)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer = 3\n",
    "scores = []\n",
    "for layer in trange(len(netparams['R_stack_sizes'])):\n",
    "    X = ztot[layer]#  ht['h{:d}'.format(layer)][:,1].reshape(tot_pure,-1) # \n",
    "#     y = labels_train['digits'].to_numpy()\n",
    "\n",
    "    # clf = svm.LinearSVC(max_iter=5000)\n",
    "    clf = make_pipeline(StandardScaler(),\n",
    "                        LinearSVC(random_state=0, tol=1e-5, max_iter=10000)) # \n",
    "\n",
    "    labels = labels_pure_df['Freq'].to_numpy()# pd.factorize(labels_train['speaker'])[0] # \n",
    "    # Split data into 50% train and 50% test subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, labels, test_size=0.5, shuffle=False, random_state=1)\n",
    "\n",
    "    # Learn the digits on the train subset\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "    print(f\"Classification report for classifier layer{layer} {clf}:\\n\"\n",
    "          f\"{metrics.classification_report(y_test, predicted)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(8,5))\n",
    "ax.plot(scores, '.-',markersize=20, linewidth=3)\n",
    "ax.axhline(y=1/len(np.unique(y_test)),linestyle='--', color='red')\n",
    "ax.set_ylim(0,1.1)\n",
    "ax.legend(['SVM','Random'],bbox_to_anchor=(1.01, 1))\n",
    "ax.set_xlabel('Layer #')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Labels=Freq')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(FigPath,'SVM_Accuracy_PureTones.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-sellers",
   "metadata": {},
   "source": [
    "# Testing Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = iter(DataLoader_Train)\n",
    "batch = batch_iter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, states, frame_pred = precnet(batch.to(device), states, grab_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_Pred = np.stack(frame_pred).transpose(1,0,2).reshape(netparams['BatchSize'], netparams['TimeSize'], netparams['height'], netparams['width'])\n",
    "\n",
    "FM_Actual = batch.squeeze().numpy()\n",
    "GT = np.squeeze(batch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT.shape, FM_Pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit,njit, prange\n",
    "import time\n",
    "\n",
    "# Calculate the error between the prediction and actual fram separated by delta t\n",
    "@njit(parallel=True)\n",
    "def parallel_diff(FM_Pred,FM_Actual,diffrange=10):\n",
    "    BatchSize,tlength,width,height = FM_Actual.shape\n",
    "    # Assumes FM_Pred and FM_Actual have same dimensions.\n",
    "    assert FM_Pred.shape == FM_Actual.shape,'Dimensions must be equal'\n",
    "    # Diff in frame loss\n",
    "    diffloss = np.zeros((tlength, diffrange*2+1))\n",
    "    diffloss[:] = np.nan\n",
    "    \n",
    "    for fm in prange(tlength):# trange(0,tlength, desc='Frame Num',leave=False): #\n",
    "        for ind, tau in enumerate(np.arange(-diffrange, diffrange+1)):\n",
    "            if ((fm+tau) <= (tlength-tau)) & ((fm+tau)>=0):\n",
    "                diffloss[fm,ind] = np.nanmean(np.abs(FM_Pred[:,fm,:,:] - FM_Actual[:,fm+tau,:,:]))\n",
    "    return diffloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffrange  = 5\n",
    "start_time = time.time()\n",
    "diffloss   = parallel_diff(FM_Pred.astype(float),GT.astype(float),diffrange=diffrange)\n",
    "end_time   = time.time()\n",
    "Tot_Time   = end_time - start_time\n",
    "print(Tot_Time)\n",
    "\n",
    "STD       = np.nanstd(diffloss,axis=0)\n",
    "DiffAvg   = np.nanmean(diffloss,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(5,5))\n",
    "x=np.arange(-diffrange,diffrange+1)+1\n",
    "ax.plot(x,DiffAvg, 'k', LineWidth=3 )\n",
    "ax.fill_between(x,DiffAvg-STD,DiffAvg+STD,alpha=.5,color='k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(frame_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_pred = np.stack(frame_pred).transpose(1,0,2).reshape(netparams['BatchSize'], netparams['TimeSize'], netparams['height'], netparams['width'])\n",
    "GT = np.squeeze(batch.detach().numpy())\n",
    "fmGT = torch.FloatTensor(GT[0]).unsqueeze(1)\n",
    "fmP = torch.FloatTensor(FM_Pred[0]).unsqueeze(1)\n",
    "GT_Grid = torchvision.utils.make_grid(fmGT,nrow=netparams['TimeSize'])\n",
    "P_Grid = torchvision.utils.make_grid(fmP,nrow=netparams['TimeSize'])\n",
    "\n",
    "fig1, axs = plt.subplots(2, 1, figsize = (20,10))\n",
    "axs[0].imshow(GT_Grid[0], aspect='auto',  origin='lower') #cmap='gray'\n",
    "axs[1].imshow(P_Grid[0],  aspect='auto',  origin='lower') #cmap='gray'\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(GT[0,1:] - FM_Pred[0,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.stack(FM_Pred[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize,tlength,width,height = FM_Actual.shape\n",
    "# Assumes FM_Pred and FM_Actual have same dimensions.\n",
    "assert FM_Pred.shape == FM_Actual.shape,'Dimensions must be equal'\n",
    "# Diff in frame loss\n",
    "diffloss = np.zeros((tlength, diffrange*2+1))\n",
    "diffloss[:] = np.nan\n",
    "\n",
    "for fm in trange(tlength):# trange(0,tlength, desc='Frame Num',leave=False): #\n",
    "    for ind, tau in enumerate(np.arange(-diffrange, diffrange+1)):\n",
    "        if ((fm+tau) <= (tlength-tau)) & ((fm+tau)>=0):\n",
    "            diffloss[fm,ind] = np.nanmean(np.abs(FM_Pred[:,fm,:,:] - FM_Actual[:,fm+tau,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = 1\n",
    "for ind, tau in enumerate(np.arange(-diffrange, diffrange+1)):\n",
    "    if ((fm+tau) <= (tlength-tau)) & ((fm+tau)>=0):\n",
    "        diffloss[fm,ind] = np.nanmean(np.abs(FM_Pred[:,fm,:,:] - FM_Actual[:,fm+tau,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0\n",
    "((fm+tau) <= (tlength-tau)), ((fm+tau)>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(np.abs(FM_Pred[:,fm,:,:] - FM_Actual[:,fm+tau,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-diffrange, diffrange+1)+1,np.nanmean(diffloss, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(diffloss, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-japan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lucky-controversy",
   "metadata": {},
   "source": [
    "# Feature Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: x[0], precnet.hd.named_children()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will register a forward hook to get the output of the layers\n",
    "\n",
    "activation = {} # to store the activation  of a layer\n",
    "def create_hook(name):\n",
    "    def hook(m, i, o):\n",
    "        # copy the output of the given layer\n",
    "        activation[name] = o\n",
    "       \n",
    "    return hook\n",
    "\n",
    "\n",
    "# register a forward hook for layer inception4a i.e. the first inception layer\n",
    "precnet.hd.register_forward_hook(create_hook('conv0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "# undo the above normalization if and when the need arises \n",
    "denormalize = T.Normalize(mean = [-0.485/0.229],  #, -0.456/0.224, -0.406/0.225],\n",
    "                                   std = [1/0.229])  #, 1/0.224, 1/0.225] )\n",
    "\n",
    "def random_image(Height = 28, Width = 28, device = 'cpu', requires_grad=False, optimizer=None, lr = 0.01):\n",
    "    img = np.single(np.random.uniform(0,1, (16, Height, Width))) # we need the pixel values to be of type float32\n",
    "    im_tensor = torch.from_numpy(img).to(device).requires_grad_(requires_grad) # normalize the image to have requisite mean and std. dev.\n",
    "    print(\"img_shape:{}, img_dtype: {}\".format(im_tensor.shape, im_tensor.dtype ))\n",
    "    \n",
    "    if optimizer:\n",
    "        if requires_grad:\n",
    "            return im_tensor, optimizer([im_tensor], lr = lr)\n",
    "        else: \n",
    "            print('Error: Optimizer cannot be used on an image without setting its requires_grad_  ')\n",
    "    \n",
    "    return im_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to massage img_tensor for using as input to plt.imshow()\n",
    "def image_converter(im):\n",
    "    \n",
    "    # move the image to cpu\n",
    "    im_copy = im.cpu()\n",
    "    \n",
    "    # for plt.imshow() the channel-dimension is the last\n",
    "    # therefore use transpose to permute axes\n",
    "    im_copy = im_copy.clone().detach().numpy().transpose(1,2,0)\n",
    "    # clip negative values as plt.imshow() only accepts \n",
    "    # floating values in range [0,1] and integers in range [0,255]\n",
    "    im_copy = im_copy.clip(0, 1) \n",
    "    \n",
    "    return im_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to compute image gradients in pytorch\n",
    "class gradients(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        k_depth, k_height, k_width = weight.shape[2:]\n",
    "        # assuming that the height and width of the kernel are always odd numbers\n",
    "        padding_x = int((k_height-1)/2)\n",
    "        padding_y = int((k_width-1)/2)\n",
    "        padding_z = int((k_depth-1)/2)\n",
    "        \n",
    "        # convolutional layer with 2 output channels corresponding to the x and the y gradients\n",
    "        self.conv = nn.Conv3d(1, 2, (k_depth,k_height, k_width), bias = False, \n",
    "                              padding = (padding_z, padding_x, padding_y) )\n",
    "        # initialize the weights of the convolutional layer to be the one provided\n",
    "        if self.conv.weight.shape == weight.shape:\n",
    "            self.conv.weight = nn.Parameter(weight)\n",
    "            self.conv.weight.requires_grad_(False)\n",
    "        else:\n",
    "            print('Error: The shape of the given weights is not correct')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradLayer = gradients(torch.from_numpy(grad_filters).unsqueeze(1).type(torch.FloatTensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 64 # height of input image\n",
    "W = 64 # width of input image\n",
    "img_tensor, optimizer = random_image(Height = H, Width = W, device = device, \n",
    "                                     requires_grad = True, optimizer = optim.Adam, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_grid = torchvision.utils.make_grid(img_tensor.cpu().unsqueeze(1),nrow=4,normalize=True)\n",
    "# # Starting Image\n",
    "plt.imshow(im_grid.permute(1,2,0))                                                               \n",
    "plt.axis('off')\n",
    "# plt.title('starting image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding the shape of the output produced by a given layer\n",
    "image = img_tensor.clone().detach().to(device).requires_grad_(True)\n",
    "model(image.unsqueeze(0).unsqueeze(0))\n",
    "activation['conv0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "num_epochs = 10000\n",
    "display_every = 1000\n",
    "unit_idx = 10 # unit of the convolution layer that we wish to visualize\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model(img_tensor.unsqueeze(0).unsqueeze(0))\n",
    "    layer_out = activation['conv0']\n",
    "    loss = -layer_out[0, unit_idx].mean() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # normalize the updated img_tensor to have pytorch specified mean and std. dev.\n",
    "    # img_tensor = normalize(img_tensor.clone().detach()).requires_grad_(True)\n",
    "    # the above step of renormalizing the updated img_tensor does not work: the activation remains frozen at ~ 2.368 and no patterns seem to develop in the image\n",
    "    # Will have to understand this further  \n",
    "    \n",
    "    if epoch % display_every == 0:\n",
    "        print('epoch: {}/{}, activation: {}'.format(epoch, num_epochs, -loss))\n",
    "        im_grid = torchvision.utils.make_grid(img_tensor.cpu().unsqueeze(1),nrow=4,normalize=True)\n",
    "        plt.imshow(im_grid.permute(1,2,0))\n",
    "        plt.axis('off')\n",
    "        plt.title('input image after {} epochs'.format(epoch))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-insulin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-knitting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "forward pass through the model to get the scores, note that VGG-19 model doesn't perform softmax at the end\n",
    "and we also don't need softmax, we need scores, so that's perfect for us.\n",
    "'''\n",
    "\n",
    "scores = model(X.to(device))[0]\n",
    "\n",
    "# Get the index corresponding to the maximum score and the maximum score itself.\n",
    "score_max_index = scores.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_max = scores[0,score_max_index]\n",
    "\n",
    "'''\n",
    "backward function on score_max performs the backward pass in the computation graph and calculates the gradient of \n",
    "score_max with respect to nodes in the computation graph\n",
    "'''\n",
    "score_max.backward()\n",
    "\n",
    "'''\n",
    "Saliency would be the gradient with respect to the input image now. But note that the input image has 3 channels,\n",
    "R, G and B. To derive a single class saliency value for each pixel (i, j),  we take the maximum magnitude\n",
    "across all colour channels.\n",
    "'''\n",
    "saliency, _ = torch.max(X.grad.data.abs(),dim=1)\n",
    "\n",
    "# code to plot the saliency map as a heatmap\n",
    "plt.imshow(saliency[0], cmap=plt.cm.hot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-leisure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scores, score_max_index, X\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-restoration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "impossible-homeless",
   "metadata": {},
   "source": [
    "# Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_path, wind_size=32, transform=None):\n",
    "        \n",
    "        self.data = np.load(data_path)\n",
    "        self.wind_size = wind_size\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.data.shape[0])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        sample = np.zeros((self.data.shape[-1]//self.wind_size,self.data.shape[-2],self.wind_size))\n",
    "        for n in range(self.data.shape[-1]//self.wind_size):\n",
    "            sample[n] = self.data[idx,:,:,(n*self.wind_size):(n*self.wind_size+self.wind_size)]\n",
    "        sample = (sample - np.min(sample))/(np.max(sample)-np.min(sample))\n",
    "        sample = np.clip(sample, 2e-30, 1).astype(np.float32)\n",
    "\n",
    "        sample = torch.Tensor(sample[:,np.newaxis])\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(data_path_train,wind_size)\n",
    "train_dataloader = DataLoader(dataset,\n",
    "                              batch_size= 60,\n",
    "                              shuffle = False,\n",
    "                              drop_last=False,\n",
    "#                               num_workers=7,\n",
    "                              pin_memory=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-safety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = iter(DataLoader_Train)\n",
    "batch = batch_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.hstack(batch[3,:,0]), aspect='auto', origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-olive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
